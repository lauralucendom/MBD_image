{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering for the facenet model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import expand_dims\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "from keras.models import Model, Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.optimizers import RMSprop, SGD\n",
    "from keras import backend as K\n",
    "import os\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import mtcnn\n",
    "from mtcnn.mtcnn import MTCNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mtcnn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the face embedding for one face\n",
    "def get_embedding(model, face_pixels):\n",
    "    # scale pixel values\n",
    "    face_pixels = face_pixels.astype('float32')\n",
    "    # standardize pixel values across channels (global)\n",
    "    mean, std = face_pixels.mean(), face_pixels.std()\n",
    "    face_pixels = (face_pixels - mean) / std\n",
    "    # transform face into one sample\n",
    "    samples = expand_dims(face_pixels, axis=0)\n",
    "    # make prediction to get embedding\n",
    "    yhat = model.predict(samples)\n",
    "    return yhat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cwd = os.getcwd()\n",
    "print(os.listdir(cwd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TensorFlow:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_face(dir):\n",
    "    faces = list()\n",
    "    # enumerate files\n",
    "    for filename in os.listdir(dir):\n",
    "        path = dir + filename\n",
    "        face = extract_face(path)\n",
    "        faces.append(face)\n",
    "    return faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dir):\n",
    "    # list for faces and labels\n",
    "    X, y = list(), list()\n",
    "    for subdir in os.listdir(dir):\n",
    "        path = dir + subdir + '/'\n",
    "        faces = load_face(path)\n",
    "        labels = [subdir for i in range(len(faces))]\n",
    "        print(\"loaded %d sample for class: %s\" % (len(faces),subdir) ) # print progress\n",
    "        X.extend(faces)\n",
    "        y.extend(labels)\n",
    "    return np.asarray(X), np.asarray(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_face(filename, required_size=(160, 160)):\n",
    "    # load image from file\n",
    "    image = Image.open(filename)\n",
    "    # convert to RGB, if needed\n",
    "    image = image.convert('RGB')\n",
    "    # convert to array\n",
    "    pixels = np.asarray(image)\n",
    "    # create the detector, using default weights\n",
    "    detector = MTCNN()\n",
    "    # detect faces in the image\n",
    "    results = detector.detect_faces(pixels)\n",
    "    # extract the bounding box from the first face\n",
    "    x1, y1, width, height = results[0]['box']\n",
    "    # deal with negative pixel index\n",
    "    x1, y1 = abs(x1), abs(y1)\n",
    "    x2, y2 = x1 + width, y1 + height\n",
    "    # extract the face\n",
    "    face = pixels[y1:y2, x1:x2]\n",
    "    # resize pixels to the model size\n",
    "    image = Image.fromarray(face)\n",
    "    image = image.resize(required_size)\n",
    "    face_array = np.asarray(image)\n",
    "    return face_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir=os.path.join(cwd, 'archive/data/train/')\n",
    "validation_data_dir=os.path.join(cwd,'archive/data/val/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "import pandas as pd\n",
    "def train_facenet(self, train_data_dir: str = 'archive/data/train/', validation_data_dir: str = 'archive/data/val/'):\n",
    "    # load train dataset\n",
    "    trainX, trainy = load_dataset(train_data_dir)\n",
    "    print(trainX.shape, trainy.shape)\n",
    "    # load test dataset\n",
    "    testX, testy = load_dataset(validation_data_dir)\n",
    "    print(testX.shape, testy.shape)\n",
    "    \n",
    "    model = load_model(\"facenet_keras.h5\")\n",
    "    print('Loaded Model')\n",
    "    print(model.inputs)\n",
    "    print(model.outputs)\n",
    "\n",
    "    # convert each face in the train set to an embedding\n",
    "    newTrainX = list()\n",
    "    for face_pixels in trainX:\n",
    "        embedding = get_embedding(model, face_pixels)\n",
    "        newTrainX.append(embedding)\n",
    "    newTrainX = np.asarray(newTrainX)\n",
    "    print(newTrainX.shape)\n",
    "    # convert each face in the test set to an embedding\n",
    "    newTestX = list()\n",
    "    for face_pixels in testX:\n",
    "        embedding = get_embedding(model, face_pixels)\n",
    "        newTestX.append(embedding)\n",
    "    newTestX = np.asarray(newTestX)\n",
    "    print(newTestX.shape)\n",
    "\n",
    "    df = pd.DataFrame(newTrainX)\n",
    "    df[\"target\"] = trainy\n",
    "    df.head()\n",
    "\n",
    "    # Create a PCA instance:\n",
    "    pca = PCA(n_components=2) \n",
    "    # Fit pca to 'X'\n",
    "    pca_features = pca.fit_transform(newTrainX)\n",
    "    print (pca_features.shape)\n",
    "\n",
    "    df_plot = pd.DataFrame(pca_features)\n",
    "    df_plot[\"target\"] = trainy\n",
    "\n",
    "    plt.figure(figsize=(16, 6))\n",
    "    sns.scatterplot(x=df_plot[0] , y= df_plot[1], data = df_plot,  hue = \"target\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load train dataset\n",
    "trainX, trainy = load_dataset(train_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(trainX.shape, trainy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load test dataset\n",
    "testX, testy = load_dataset(validation_data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(testX.shape, testy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model(\"facenet_keras.h5\")\n",
    "print('Loaded Model')\n",
    "print(model.inputs)\n",
    "print(model.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert each face in the train set to an embedding\n",
    "newTrainX = list()\n",
    "for face_pixels in trainX:\n",
    "    embedding = get_embedding(model, face_pixels)\n",
    "    newTrainX.append(embedding)\n",
    "newTrainX = np.asarray(newTrainX)\n",
    "print(newTrainX.shape)\n",
    "# convert each face in the test set to an embedding\n",
    "newTestX = list()\n",
    "for face_pixels in testX:\n",
    "    embedding = get_embedding(model, face_pixels)\n",
    "    newTestX.append(embedding)\n",
    "newTestX = np.asarray(newTestX)\n",
    "print(newTestX.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame(newTrainX)\n",
    "df[\"target\"] = trainy\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot as plt\n",
    "# Create a PCA instance:\n",
    "pca = PCA(n_components=2) \n",
    "# Fit pca to 'X'\n",
    "pca_features = pca.fit_transform(newTrainX)\n",
    "print (pca_features.shape)\n",
    "\n",
    "df_plot = pd.DataFrame(pca_features)\n",
    "df_plot[\"target\"] = trainy\n",
    "\n",
    "plt.figure(figsize=(16, 6))\n",
    "sns.scatterplot(x=df_plot[0] , y= df_plot[1], data = df_plot,  hue = \"target\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering for the model_dropout_05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenvNE",
   "language": "python",
   "name": "myenvne"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}